# Single-Replica Design

This operator is designed for **single-replica deployment only**. Leader election has been removed for simplicity.

---

## âš ï¸ **IMPORTANT: Single Replica Only**

```yaml
replicaCount: 1  # DO NOT CHANGE!
```

**This operator does NOT support multiple replicas.**

Running multiple replicas will cause:
- âŒ Duplicate backend CRD creation
- âŒ Conflicting updates
- âŒ Race conditions
- âŒ Resource thrashing

---

## âœ… **Why Single Replica?**

### **Design Decision:**

Leader election was removed to **simplify** the operator:

1. **Simpler Code**
   - No lease management
   - No coordination logic
   - No standby overhead

2. **Lower Resource Usage**
   - Single pod only
   - No coordination API calls
   - Minimal footprint

3. **Easier to Debug**
   - Single source of logs
   - No "which pod is leader?" questions
   - Clear reconciliation path

4. **Sufficient for Most Use Cases**
   - Development and testing
   - Small to medium deployments
   - Non-critical workloads

---

## ğŸ”’ **How Single Replica is Enforced**

### **1. Deployment Strategy: Recreate**

```yaml
# values.yaml
strategy:
  type: Recreate
```

**What this does:**
- During upgrades: Old pod terminates **first**
- Then: New pod starts
- **Never** two pods running simultaneously
- Brief downtime (~10-30 seconds) during upgrades

**Alternative (RollingUpdate) NOT used:**
```yaml
strategy:
  type: RollingUpdate  # â† Would allow 2 pods temporarily
```

### **2. Replica Count: 1**

```yaml
# values.yaml
replicaCount: 1
```

**Hard-coded in Helm chart.**

### **3. Warning Comments**

```yaml
# WARNING: This operator is designed for single-replica deployment only
# Leader election is disabled - do NOT scale beyond 1 replica
```

---

## ğŸ“Š **Comparison: Single vs Multi-Replica**

| Aspect | Single Replica (Current) | Multi-Replica (With Leader Election) |
|--------|--------------------------|--------------------------------------|
| **High Availability** | âŒ No | âœ… Yes (~15s failover) |
| **Complexity** | âœ… Low | âŒ Higher |
| **Resource Usage** | âœ… Minimal | âŒ More (multiple pods) |
| **Upgrade Downtime** | âš ï¸ Yes (~30s) | âœ… None (rolling) |
| **Conflicts** | âœ… None | âœ… None (via leader election) |
| **Debugging** | âœ… Easy | âŒ Harder (which pod?) |
| **Good For** | Dev, test, small prod | Large prod, critical workloads |

---

## âš™ï¸ **Deployment Details**

### **Normal Deployment:**

```bash
helm install unified-replication-operator ./helm/unified-replication-operator \
  --namespace unified-replication-system \
  --create-namespace
```

**Result:**
- 1 pod created
- No leader election
- Simple operation

### **Upgrade Process:**

```bash
helm upgrade unified-replication-operator ./helm/unified-replication-operator \
  --namespace unified-replication-system
```

**What happens:**
```
Time 0:   Old pod receives SIGTERM
Time 2s:  Old pod gracefully shuts down
Time 3s:  Old pod terminated
Time 4s:  New pod starts
Time 15s: New pod ready
Time 16s: Operator reconciling again
```

**Downtime:** ~10-30 seconds (acceptable for most workloads)

---

## âš ï¸ **What NOT to Do**

### **âŒ DO NOT Scale Beyond 1 Replica**

```bash
# This will cause problems!
kubectl scale deployment unified-replication-operator -n unified-replication-system --replicas=3
```

**Problems:**
- All 3 pods reconcile simultaneously
- Duplicate backend CRD creation
- Conflicting updates
- Resource waste

### **âŒ DO NOT Use RollingUpdate Strategy**

```yaml
# DO NOT change this!
strategy:
  type: RollingUpdate  # â† WRONG! Allows 2 pods during update
```

---

## âœ… **What You CAN Do**

### **1. Configure Resource Limits**

```yaml
# values.yaml
resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 100m
    memory: 128Mi
```

Safe to adjust based on your workload.

### **2. Add Node Affinity**

```yaml
# values.yaml
nodeSelector:
  node-role.kubernetes.io/worker: ""
```

Pin to specific nodes if needed.

### **3. Use PodDisruptionBudget**

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: unified-replication-operator
spec:
  maxUnavailable: 0  # Prevent voluntary disruptions
  selector:
    matchLabels:
      app.kubernetes.io/name: unified-replication-operator
```

Prevents node drains during maintenance (pod must be rescheduled first).

---

## ğŸ” **Monitoring Single Replica**

### **Check Pod Health:**

```bash
kubectl get pods -n unified-replication-system

# Expected: 1 pod, Running
# NAME                                            READY   STATUS
# unified-replication-operator-xxxxx-yyyyy        1/1     Running
```

### **What if Pod Crashes?**

```
Pod crashes â†’ Kubernetes restarts it automatically
Restart time: ~10-30 seconds
During restart: No reconciliation (brief gap)
After restart: All replications reconciled
```

**Recovery is automatic via Kubernetes restart policy.**

### **Check Reconciliation:**

```bash
# View logs
kubectl logs -n unified-replication-system -l control-plane=controller-manager -f

# Check replications being reconciled
kubectl get uvr -A -w
```

---

## ğŸ“‹ **Troubleshooting**

### **Issue: Pod Not Starting**

```bash
# Check pod status
kubectl describe pod -n unified-replication-system -l control-plane=controller-manager

# Check logs
kubectl logs -n unified-replication-system -l control-plane=controller-manager
```

### **Issue: Reconciliation Slow**

Single replica handles all replications:

```bash
# Check how many replications
kubectl get uvr -A | wc -l

# If too many (>100), consider:
# - Adjusting reconciliation concurrency
# - Splitting across namespaces
# - OR re-enabling leader election with multiple replicas
```

### **Issue: Upgrade Causing Issues**

Brief downtime is expected:

```bash
# Check upgrade status
kubectl rollout status deployment unified-replication-operator \
  -n unified-replication-system

# Check new pod started
kubectl get pods -n unified-replication-system
```

---

## ğŸ¯ **When to Consider Leader Election**

Consider re-enabling leader election if:

- âœ… You have > 100 replications
- âœ… You need 24/7 availability
- âœ… You can't tolerate upgrade downtime
- âœ… You need automatic failover

**To re-enable:**
1. Uncomment leader election code in `main.go`
2. Add back leader election RBAC
3. Set `replicaCount: 3` in values
4. Change strategy to `RollingUpdate`

---

## ğŸ“Š **Production Readiness**

### **Single Replica is Production-Ready For:**

âœ… **Development/Staging**
- Testing and validation
- CI/CD pipelines
- Non-critical environments

âœ… **Small Production**
- < 50 replications
- Can tolerate brief downtime
- Lower complexity preferred

âœ… **Cost-Sensitive**
- Minimal resource usage
- Single pod overhead
- Lower cloud costs

### **NOT Production-Ready For:**

âŒ **Large-Scale Production**
- > 100 replications
- High reconciliation volume
- 24/7 uptime requirements

âŒ **Critical Workloads**
- Zero downtime requirements
- Automatic failover needed
- Multiple availability zones

---

## ğŸš€ **Current Configuration**

```yaml
# Enforced Settings
replicaCount: 1          # Single pod only
strategy.type: Recreate  # No overlap during updates
LeaderElection: false    # No coordination needed

# Effects
- One active pod at all times
- No leader election overhead
- Brief downtime during upgrades
- Simpler operation
```

---

## ğŸ“– **Summary**

### **Design Choice:**
**Single-replica deployment** with leader election removed

### **Rationale:**
- Simplicity over high availability
- Sufficient for most use cases
- Lower operational overhead

### **Constraints:**
- **DO NOT scale beyond 1 replica**
- **Expect downtime during upgrades** (~30s)
- **No automatic failover**

### **Benefits:**
- âœ… Simpler deployment
- âœ… Lower resource usage
- âœ… Easier to debug
- âœ… No coordination overhead

**This design choice prioritizes simplicity and is appropriate for development, testing, and small-to-medium production deployments.** ğŸ¯

---

*Design Decision: Single Replica*  
*Leader Election: Disabled*  
*Last Updated: 2025-10-20*  
*Operator Version: 0.3.0*

